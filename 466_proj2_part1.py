# -*- coding: utf-8 -*-
"""466-proj2-part1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qw9ktmDdSpLdZfTVtl6yg3w54lpRIMWr
"""

path = "drive/My Drive/Colab Notebooks/466-proj2/"
f = open(path + 'readme.txt', 'r')
print(f.read())
f.close()

FILE = "committee_utterances.tsv"

import pandas as pd

df = pd.read_csv(path + FILE, sep='\t')
df

#Select a random 25% (1/4) of the content
import random
from pprint import pprint

records = list(df.text)
number_selected = len(records) // 100  #TODO CHANGE THIS VALUE TO 4
selected_records = random.sample(records, number_selected) #get random sample of number_selected records without replacement

utterances = [record for record in selected_records] #get all the utterance fields only (column #15) for clustering
print("total number of points:",len(utterances))

import nltk
from nltk.stem import WordNetLemmatizer
from nltk.tokenize import sent_tokenize, word_tokenize
from nltk.corpus import stopwords 
from nltk.corpus import wordnet 
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('wordnet')
nltk.download('averaged_perceptron_tagger')

wordnet_lemmatizer = WordNetLemmatizer()
stop_words = set(stopwords.words('english')) 
punctuation = "-!?;:\"\'.,"

def getPOS(word):
    """Map POS tag to first character lemmatize() accepts"""
    tag = nltk.pos_tag([word])[0][1][0].upper()
    tag_dict = {"J": wordnet.ADJ,
                "N": wordnet.NOUN,
                "V": wordnet.VERB,
                "R": wordnet.ADV}

    return tag_dict.get(tag, wordnet.NOUN)

def getFeatures(text):
    features = {}
    tokens = nltk.word_tokenize(text.lower())
    tot_cnt = len(tokens)
    for word in tokens:        
        if word not in stop_words and word not in punctuation:
            w = wordnet_lemmatizer.lemmatize(word, getPOS(word))
            if w not in features:
                features[w] = 1/float(tot_cnt)
            else:
                features[w] += 1/float(tot_cnt)
    return features

import numpy as np
import random, sys
from collections import Counter

max_t = 2   # max itterations (ignores threshold) (-1 to unset)
k = 2
M = []
M.append([0 for i in range(0, k)])

def initCentroids(vec_data, k):
    centroidID = np.random.permutation(len(vec_data))[:k]
    return [id for id in centroidID]

def calculateDistance(v_dict, u_dict):
    v = list(v_dict.values())
    u = list(u_dict.values())
    total = 0
    for i in range(0, len(v)):
        total += ((v[i] - u[i]) ** 2)
    return np.sqrt(total)


# takes a vector dictionary and a set of all the keys
# returns a new vector dictionary with all the keys
def reshapeVectDict(vect_dict, keys):
    old_shape = len(vect_dict)
    for key in keys:
        if key not in vect_dict:
            vect_dict[key] = 0.0
    return vect_dict

# For a given point, find the closest centroid from the list, return centroid's index
def closestCentroid(point, centroids, data):
    # Get all the keys (words) in the point and centroids
    # This will be used to transform them to the same dimension so we can do math
    all_keys = []
    all_keys.extend(point)        
    for centroid in centroids:        
        all_keys.extend(data[centroid].keys())
    all_keys = set(all_keys)

    closestCentroid = 0
    minDist = sys.maxsize
    point_reshape = reshapeVectDict(point, all_keys)

    for cID in centroids:
        centroid = data[cID]
        centroid_reshape = reshapeVectDict(centroid, all_keys)
        dist = calculateDistance(point_reshape, centroid_reshape)

        if (dist < minDist):
            minDist = dist          
            closestCentroid = cID

    return closestCentroid


def k_means(vec_data, k, e):
    t = 0
    M[t] = initCentroids(vec_data, k)

    if max_t >= 0:        
        while (t < max_t):
            print(t)
            t += 1
            M.append([0 for i in range(0, k)])    # M needs t rows
            C = []
            for i in range(0, k):
                C.append([])
            # Centroid assignment
            for point in vec_data:
                clusterID = closestCentroid(point, M[t-1], vec_data)
                index = M[t-1].index(clusterID)
                C[index].append(point)
            # Centroid update
            # for i in range(0, k):
            #     M[t][i] =

data = selected_records
vec_data = [getFeatures(text) for text in data]
print(len(vec_data))

import time

startTime = time.time()
endTime = time.time()

k_means(vec_data, 2, 1)

print(round(endTime - startTime,3),'seconds')

# M holds the centroid IDs
M