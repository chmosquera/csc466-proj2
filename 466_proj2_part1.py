# -*- coding: utf-8 -*-
"""466-proj2-part1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qw9ktmDdSpLdZfTVtl6yg3w54lpRIMWr
"""

import nltk
from nltk.stem import WordNetLemmatizer
from nltk.tokenize import sent_tokenize, word_tokenize
from nltk.corpus import stopwords 
from nltk.corpus import wordnet 
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('wordnet')
nltk.download('averaged_perceptron_tagger')

wordnet_lemmatizer = WordNetLemmatizer()
stop_words = set(stopwords.words('english')) 
punctuation = "-!?;:\"\'.,"

def getPOS(word):
    """Map POS tag to first character lemmatize() accepts"""
    tag = nltk.pos_tag([word])[0][1][0].upper()
    tag_dict = {"J": wordnet.ADJ,
                "N": wordnet.NOUN,
                "V": wordnet.VERB,
                "R": wordnet.ADV}

    return tag_dict.get(tag, wordnet.NOUN)

def getFeatures(text):
    features = {}
    tokens = nltk.word_tokenize(text.lower())
    tot_cnt = len(tokens)
    for word in tokens:        
        if word.isnumeric():
            label = '[NUMBER]'
            if label not in features:
                features[label] = 1/float(tot_cnt)    
            else:
                features[label] += 1/float(tot_cnt)    
        elif word not in stop_words and word not in punctuation:
            w = wordnet_lemmatizer.lemmatize(word, getPOS(word))
            if w not in features:
                features[w] = 1/float(tot_cnt)
            else:
                features[w] += 1/float(tot_cnt)
    return features

import numpy as np
import random, sys, copy
from collections import Counter

def initCentroids(vec_data, k):
    centroidID = np.random.permutation(len(vec_data))[:k]
    # print(centroidID)
    return [vec_data[cID] for cID in centroidID]

def calculateDistance(v_dict, u_dict):
    v = list(v_dict.values())
    u = list(u_dict.values())
    total = 0
    for i in range(0, len(v)):
        total += ((v[i] - u[i]) ** 2)
    return np.sqrt(total)


# takes a vector dictionary and a set of all the keys
# returns a new vector dictionary with all the keys
def reshapeVectDict(vect_dict, keys):
    result = copy.deepcopy(vect_dict)
    for key in keys:
        if key not in result:
            result[key] = 0.0
    return result

# For a given point, find the closest centroid from the list, return centroid's index
def closestCentroid(point, k, data):
    # Get all the keys (words) in the point and centroids
    # This will be used to transform them to the same dimension so we can do math
    all_keys = []
    all_keys.extend(point)        
    for cID in range(0,k):        
        all_keys.extend(data[cID].keys())
    all_keys = set(all_keys)

    closestCentroid = 0
    minDist = sys.maxsize
    point_reshape = reshapeVectDict(point, all_keys)

    # print()
    # print("point: ", point_reshape)
    for cID in range(0,k):
        centroid = data[cID]
        centroid_reshape = reshapeVectDict(centroid, all_keys)
        dist = calculateDistance(point_reshape, centroid_reshape)

        # print("dist: ", dist, "centroid: ", centroid)
        if (dist < minDist):
            minDist = dist          
            closestCentroid = cID

    # print("closestCentroid: ", closestCentroid, "minDist: ", minDist)
    return closestCentroid

def recalculateCentroid(cluster, data):
    new_centroid = {}
    for idx in cluster:
        point = data[idx]
        new_centroid = dict(Counter(new_centroid) + Counter(point))

    for key in new_centroid:
        new_centroid[key] /= len(cluster)
    # print("new_centroid: ", new_centroid)
    return new_centroid

def calculateConvergence(k, t, M):
    difference = 0
    for i in range(0, k):
        v = list(M[t][i].values())
        u = list(M[t-1][i].values())
        diff = (np.mean(u) - np.mean(v)) ** 2
        difference += diff    
    print(difference)
    return difference

def k_means(vec_data, k, e):
    M = []
    M.append([0 for i in range(0, k)])

    t = 0
    M[t] = initCentroids(vec_data, k)

    cluster = []
    condition = True
    while(condition):
        # print("iteration: ", t)
        t += 1
        M.append([0 for i in range(0, k)])    # M needs t rows
        C = []
        for i in range(0, k):
            C.append([])
        # Centroid assignment
        for point in vec_data:
            clusterID = closestCentroid(point, k, M[t-1])                
            C[clusterID].append(vec_data.index(point))    # keep track of the point's as indices
        
        # Centroid update
        for i in range(0, k):
            new_centroid = recalculateCentroid(C[i], vec_data)
            M[t][i] = new_centroid 

        cluster = C
        condition = calculateConvergence(k, t, M) > e
    
    return cluster

# Takes a list of vect-dicts that describe the cluster, and converts the data to a list of their cluster labels

def printClusters(clusters):
    result = []
    for IDX in range(0, len(clusters)):
        print("Cluster ", IDX, " contains:")
        samples = [vec_data[i] for i in clusters[IDX]]
        print(samples)
        result.append(samples)
    return result
    
def getClusterLabel(item, clusters):
    for cluster in clusters:
        if item in cluster:
            return clusters.index(cluster)

def summarizeClusters(clusters):
    summary = []
    for cluster in clusters:
        summary.append(len(cluster))
    return [cid for cid in range(0, len(clusters))], summary

import pandas as pd

FILE = "committee_utterances.tsv"
path = "drive/My Drive/Colab Notebooks/466-proj2/"

df = pd.read_csv(path + FILE, sep='\t')

#Select a random 25% (1/4) of the content
records = list(df.text)
number_selected = len(records) // 16  #TODO CHANGE THIS VALUE TO 4
selected_records = random.sample(records, number_selected) #get random sample of number_selected records without replacement

data = selected_records

# vectorize data
vec_data = [getFeatures(text) for text in data]

# k-means clustering
K = 5
E = .00000001 # threshold
clusters = k_means(vec_data, K, E)

cluster_labels, cluster_cnts = summarizeClusters(clusters)
print("Cluster IDs: ", cluster_labels)
print("# of items in each:")
for idx in range(len(cluster_cnts)):
    print(idx, "-", cluster_cnts[idx])

"""#Evluation"""

# Vectorize features
from sklearn.feature_extraction import DictVectorizer

vectorizer = DictVectorizer()
X_vec = vectorizer.fit_transform(map(getFeatures, data))

from sklearn.cluster import KMeans

kmeans = KMeans(n_clusters=5, max_iter=600, algorithm = 'auto')
fitted = kmeans.fit(X_vec)

# set up contingency table
contingency_table = []
for t in range(0, len(clusters)):
    row = [0 for c in range(0, kmeans.n_clusters)]
    contingency_table.append(row)

inverse = vectorizer.inverse_transform(X_vec)

# add counts to contingency table
for item_idx in range(0, len(vec_data)):
    t_label = getClusterLabel(item_idx, clusters)
    c_label = fitted.labels_[inverse.index(vec_data[item_idx])]
    if t_label is not None and c_label is not None:   # i have an error where some cluster labels are NONE
        contingency_table[t_label][c_label] +=1

from tabulate import tabulate

print(tabulate(contingency_table))

summarizeClusters(clusters)

result = printClusters(clusters)

pprint(result[0])

from collections import Counter
cluster_0 = {}
for text_id in clusters[0]:
    cluster_0 = Counter(vec_data[text_id]) + Counter(cluster_0)

sorted(cluster_0.items(), key=lambda x: x[1], reverse=True)

cluster_2 = {}
for text_id in clusters[2]:
    cluster_2 = Counter(vec_data[text_id]) + Counter(cluster_2)

sorted(cluster_2.items(), key=lambda x: x[1], reverse=True)

cluster_3 = {}
for text_id in clusters[3]:
    cluster_3 = Counter(vec_data[text_id]) + Counter(cluster_3)

sorted(cluster_3.items(), key=lambda x: x[1], reverse=True)

cluster_4 = {}
for text_id in clusters[4]:
    cluster_4 = Counter(vec_data[text_id]) + Counter(cluster_4)

sorted(cluster_4.items(), key=lambda x: x[1], reverse=True)